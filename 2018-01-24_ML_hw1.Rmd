---
title: "Machine Learning HW1 (Perceptron & Winnow)"
author: "Daniel Truver"
date: "2018/01/24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(scatterplot3d)
library(dplyr)
```

#### Perceptron Algorithm and Convergence Analysis

#####(1) 
We consider Boolean functions $f: \{0,1\}^n \rightarrow \{0,1\} $  
(a) Consider  

$$
y = f(x_1, x_2) = x_1 \text{ AND } x_2 = x_1 \cdot x_2
$$

Then, $y = 1$ iff $x_1 = x_2 = 1$. We can separate these points with a line in the plane.  

```{r, echo=FALSE}
# vectors with the coordinates of interest
x_1 = c(0,0,1,1)
x_2 = c(0,1,0,1)
y = x_1*x_2
y_labs = paste0("y = ", y)
# plotting the points with values attached
ggplot(data = data.frame(x_1, x_2, y_labs), aes(x = x_1, y = x_2)) +
  geom_point() + 
  geom_label(aes(label = y_labs), nudge_y = 0.1) + 
  geom_abline(slope = -1, intercept = 1.5, color = "red")+
  ggtitle("Function (x_1 AND x_2) with Separator (in red)")+
  xlim(0,1.2) +
  ylim(0,1.2) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

(b) Consider  

$$
y = f(x_1, x_2) = x_1 \text{ XOR } x_2 = x_1 + x_2 \text{(mod 2)}
$$

Then, for $(x_1, x_2) \in \{0,1\}^2 $, $y = 1$ iff $x_1 \neq x_2$. Graphically, we can see there is no separator for points where $y = 0$ and $y=1$ since $(0,1), (1,0)$ are colinear with $(0,0), (1,1)$ on opposite sides of the shared line.   

```{r, echo = FALSE}
# coordinates have not changed, just y has
y = (x_1 + x_2) %% 2
x_prime = exp(y)
y_labs = paste0("y = ", y)
# plot the points. So fun!
ggplot(data = data.frame(x_1, x_2, y_labs), aes(x_1, x_2)) + 
  geom_point() + 
  geom_abline(slope = -1, intercept = 1, color = "red")+
  geom_label(aes(label = y_labs), nudge_y = 0.1) + 
  ggtitle("Function (x_1 XOR x_2) with Illustration of Non-separability")+
  xlim(0,1.2) +
  ylim(0,1.2) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


(c) Consider   

$$
y = f(x_1,x_2,x_3) = x_1 \text{ AND } x_2 \text{ AND } x_3 = x_1 \cdot x_2 \cdot x_3
$$

Then, we have a case similar to (a) where $y=1 \iff x_1 = x_2 = x_3 = 1$.  

```{r, echo=FALSE}
# create the vectors of coordinates
x_1 = c(rep(0,4), rep(1,4))
x_2 = c(0,0,1,1,0,0,1,1)
x_3 = c(0,1,0,1,0,1,0,1)
y = x_1 * x_2 * x_3
y_labs = paste0("y = ", y)
# plot those SOB's
{
  test = scatterplot3d(x_1, x_2, x_3, color = y + 1, pch = 16, cex.symbols = 1.5)
  test$plane3d(2.1,-1, -1)
}
```

#####(2)

Since our classifier is $sign(f(x))$, the decision boundary is the hyperplane $0 = \beta_0 + \beta^Tx = \beta_0 + \beta_1x_1 + \dots + \beta_px_p$. From calculus, at least for me, we know the distance between a point $x_0$ and a hyperplane is the projection of $x - x_0$, for $x$ in the plane, onto the normal vector of the plane, here given by $\beta$. Let $B$ denote the decision boundary, $x$ denote a point on $B$, and $x_0$ denote an fixed point. The signed Euclidean distance is then:
$$
\begin{aligned}
D(x_0, B) 
&= proj_\beta(x_0-x) \\
&= \beta \cdot (x_0-x)/||\beta||_2 \\ 
&= (\beta^Tx_0 - \beta^Tx)/||\beta||_2 \\
&= (\beta_0 + \beta^Tx_0)/||\beta||_2 \quad \text{since }x\text{ is a point on }B \\
&= f(x_0)/||\beta||_2 
\end{aligned}
$$

Multiplying both sides by $y$, since $y$ is given for $x$ in this problem, we get $yD(x, B) = yf(x)/||\beta||_2 $. If $x$ is misclassified, then the sign of $y$ will differ from the sign of $f(x)$ and this value will be negative. Therefore, this quantity is the margin. 

#####(3)
I feel like I'm missing something here.  

The perceptron initializes at $w^{(0)} = 0 $, so $||w^{(0)} - w^{sep}||_2^2 = ||w^{sep}||^2_2 $. If we normalize $w^{sep}$ by dividing by its magnitude and recaculate the margin, then we get $y_i(x_i\cdot w^{sep})/||w^{sep}||_2 \geq 1/||w^{sep}|| $ and we have met the conditions of the perceptron convergence bound theorem proved in class. Note, even if $\exists i : ||x_i|| > 1$, rescaling would change the bound to $1/(||w^{sep}||\cdot ||x_i||) < 1/||w^{sep}|| $, so $1/||w^{sep}||$ is still a bound, and the theorem gives us $T < ||w^{sep}||^2$, what was to be shown.  

#### Programming Assignment (using R)  

Oh dear, data loading.  

```{r loadMNIST}
# special thanks to: https://gist.github.com/brendano/39760
# Load the MNIST digit recognition dataset into R
# http://yann.lecun.com/exdb/mnist/
# assume you have all 4 files and gunzip'd them
# creates train$n, train$x, train$y  and test$n, test$x, test$y
# e.g. train$x is a 60000 x 784 matrix, each row is one digit (28x28)
# call:  show_digit(train$x[5,])   to see a digit.
# brendan o'connor - gist.github.com/39760 - anyall.org

load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('mnist/train-images-idx3-ubyte')
  test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
  
  train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
  test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')  
}
load_mnist()
```

Now that we're done with the worst part--dear god, 200MB of memory--we proceed with the actual assignment. 