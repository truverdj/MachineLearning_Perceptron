---
title: "Machine Learning HW1 (Perceptron & Winnow)"
author: "Daniel Truver"
date: "2018/01/24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(scatterplot3d)
library(dplyr)
```

#### Perceptron Algorithm and Convergence Analysis

#####(1) 
We consider Boolean functions $f: \{0,1\}^n \rightarrow \{0,1\} $  
(a) Consider  

$$
y = f(x_1, x_2) = x_1 \text{ AND } x_2 = x_1 \cdot x_2
$$

Then, $y = 1$ iff $x_1 = x_2 = 1$. We can separate these points with a line in the plane.  

```{r, echo=FALSE}
# vectors with the coordinates of interest
x_1 = c(0,0,1,1)
x_2 = c(0,1,0,1)
y = x_1*x_2
y_labs = paste0("y = ", y)
# plotting the points with values attached
ggplot(data = data.frame(x_1, x_2, y_labs), aes(x = x_1, y = x_2)) +
  geom_point() + 
  geom_label(aes(label = y_labs), nudge_y = 0.1) + 
  geom_abline(slope = -1, intercept = 1.5, color = "red")+
  ggtitle("Function (x_1 AND x_2) with Separator (in red)")+
  xlim(0,1.2) +
  ylim(0,1.2) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

(b) Consider  

$$
y = f(x_1, x_2) = x_1 \text{ XOR } x_2 = x_1 + x_2 \text{(mod 2)}
$$

Then, for $(x_1, x_2) \in \{0,1\}^2 $, $y = 1$ iff $x_1 \neq x_2$. Graphically, we can see there is no separator for points where $y = 0$ and $y=1$ since $(0,1), (1,0)$ are colinear with $(0,0), (1,1)$ on opposite sides of the shared line.   

```{r, echo = FALSE}
# coordinates have not changed, just y has
y = (x_1 + x_2) %% 2
x_prime = exp(y)
y_labs = paste0("y = ", y)
# plot the points. So fun!
ggplot(data = data.frame(x_1, x_2, y_labs), aes(x_1, x_2)) + 
  geom_point() + 
  geom_abline(slope = -1, intercept = 1, color = "red")+
  geom_label(aes(label = y_labs), nudge_y = 0.1) + 
  ggtitle("Function (x_1 XOR x_2) with Illustration of Non-separability")+
  xlim(0,1.2) +
  ylim(0,1.2) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


(c) Consider   

$$
y = f(x_1,x_2,x_3) = x_1 \text{ AND } x_2 \text{ AND } x_3 = x_1 \cdot x_2 \cdot x_3
$$

Then, we have a case similar to (a) where $y=1 \iff x_1 = x_2 = x_3 = 1$.  

```{r, echo=FALSE}
# create the vectors of coordinates
x_1 = c(rep(0,4), rep(1,4))
x_2 = c(0,0,1,1,0,0,1,1)
x_3 = c(0,1,0,1,0,1,0,1)
y = x_1 * x_2 * x_3
y_labs = paste0("y = ", y)
# plot those SOB's
{
  test = scatterplot3d(x_1, x_2, x_3, color = y + 1, pch = 16, cex.symbols = 1.5)
  test$plane3d(2.1,-1, -1)
}
```

#####(2)

Since our classifier is $sign(f(x))$, the decision boundary is the hyperplane $0 = \beta_0 + \beta^Tx = \beta_0 + \beta_1x_1 + \dots + \beta_px_p$. From calculus, at least for me, we know the distance between a point $x_0$ and a hyperplane is the projection of $x - x_0$, for $x$ in the plane, onto the normal vector of the plane, here given by $\beta$. Let $B$ denote the decision boundary, $x$ denote a point on $B$, and $x_0$ denote an fixed point. The signed Euclidean distance is then:
$$
\begin{aligned}
D(x_0, B) 
&= proj_\beta(x_0-x) \\
&= \beta \cdot (x_0-x)/||\beta||_2 \\ 
&= (\beta^Tx_0 - \beta^Tx)/||\beta||_2 \\
&= (\beta_0 + \beta^Tx_0)/||\beta||_2 \quad \text{since }x\text{ is a point on }B \\
&= f(x_0)/||\beta||_2 
\end{aligned}
$$

Multiplying both sides by $y$, since $y$ is given for $x$ in this problem, we get $yD(x, B) = yf(x)/||\beta||_2 $. If $x$ is misclassified, then the sign of $y$ will differ from the sign of $f(x)$ and this value will be negative. Therefore, this quantity is the margin. 

#####(3)
I feel like I'm missing something here.  

The perceptron initializes at $w^{(0)} = 0 $, so $||w^{(0)} - w^{sep}||_2^2 = ||w^{sep}||^2_2 $. If we normalize $w^{sep}$ by dividing by its magnitude and recaculate the margin, then we get $y_i(x_i\cdot w^{sep})/||w^{sep}||_2 \geq 1/||w^{sep}|| $ and we have met the conditions of the perceptron convergence bound theorem proved in class. Note, even if $\exists i : ||x_i|| > 1$, rescaling would change the bound to $1/(||w^{sep}||\cdot ||x_i||) < 1/||w^{sep}|| $, so $1/||w^{sep}||$ is still a bound, and the theorem gives us $T < ||w^{sep}||^2$, what was to be shown.  

#### Programming Assignment (using R)  

Oh dear, data loading and data cleaning.  

```{r loadMNIST}
# special thanks to: https://gist.github.com/brendano/39760
# Load the MNIST digit recognition dataset into R
# http://yann.lecun.com/exdb/mnist/
# assume you have all 4 files and gunzip'd them
# creates train$n, train$x, train$y  and test$n, test$x, test$y
# e.g. train$x is a 60000 x 784 matrix, each row is one digit (28x28)
# call:  show_digit(train$x[5,])   to see a digit.
# brendan o'connor - gist.github.com/39760 - anyall.org

load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('mnist/train-images-idx3-ubyte')
  test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
  
  train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
  test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')  
}
load_mnist()
```

Now that we're done with the worst loading--dear god, 200MB of memory--we proceed with data cleaning.  

```{r cleanMNIST}
# get observations of 4 or 9
keep_train = (train$y == 4 | train$y == 9) 
keep_test = (test$y == 4 | test$y == 9)
# subset the data
x_train = train$x[keep_train,]
y_train = train$y[keep_train] 

x_test = test$x[keep_test,]
y_test = test$y[keep_test]
# norm function
vnorm = function(x){sqrt(sum(x^2))}
train_norms = apply(x_train, 1, vnorm)
x_train = x_train/max(train_norms)
# recode the response so 9 is 1, 4 is -1
recode_1 = which(y_train == 9)
y_train[recode_1] = 1
y_train[-recode_1] = -1
n_train = length(y_train)
#testing set
test_norms = apply(x_test, 1, vnorm)
x_test = x_test/max(test_norms)
#recode test response
recode_1 = which(y_test == 9)
y_test[recode_1] = 1
y_test[-recode_1] = -1
n_test = length(y_test)
```

Now that the data is in the format we like, $||x_i||_2 \leq 1, y \in \{-1,1\}$, we can get started on the perceptron.  

##### (1) Perceptron

```{r perceptronFunction}
perceptron = function(X, y, I){
  accuracy_epoch = data.frame(matrix(NA, nrow = I, ncol = 2)) # accuracy-epoch matrix
  colnames(accuracy_epoch) = c("accuracy", "epoch")
  W = matrix(NA, nrow = I, ncol = ncol(X))
  w = rep(0, dim(X)[2])
  for (j in 1:I){
    for (i in seq_along(y)){
      if (y[i]*(w %*% X[i,]) <= 0){
        w = w + y[i]*X[i,] # update step
      }
    }
    accuracy = sum((X %*% w) * y > 0)/length(y) # calculate propotion of correctly classified points
    accuracy_epoch[j, "accuracy"] = accuracy
    accuracy_epoch[j, "epoch"] = j 
    W[j,] = w
    if (accuracy == 1){ 
      break # no need to continue if we have perfect separation
    } 
  }
  return(list("a.e" = accuracy_epoch, "w" = w, "W" = W))
}
```

(a) Running the perceptron on the training set.  

```{r perceptronTraining}
res_train = perceptron(X = x_train, y = y_train, I = 100)
```

That was faster than expected, let's look at these delicious accuracy-epoch plots.  

```{r accuracyPlotTraining, echo=FALSE}
g1 = ggplot(data = res_train$a.e, aes(x = epoch, y = accuracy)) +
  geom_line() + 
  ggtitle("How Accuracy Changes w.r.t. Epoch", subtitle = "Training Data") +
  theme_bw() +
  xlab("Epoch") + 
  ylab("Accuracy") +
  ylim(0,1) +
  geom_hline(yintercept = min(res_train$a.e$accuracy), lty = "dashed") + 
  geom_hline(yintercept = max(res_train$a.e$accuracy), lty = "dashed")
g1
```

Suprisingly, at least to me, we see here that accuracy is quite high after 1 epoch, not monotonic, and stays close to 0.96. The actual range is [`r min(res_train$a.e$accuracy)`,`r max(res_train$a.e$accuracy)`]. It reaches its maximum value at epoch `r which.max(res_train$a.e$accuracy) ` and its minimum at epoch `r which.min(res_train$a.e$accuracy) `. From this plot, it does not seem that increasing or decreasing the maximum number of epochs will have much of an effect on the accuracy. Hmm...suspicious.  

(b) Since this is the test set, and I'm usually a test set purist, I'm going to interpret the question to mean we take the values of $w$ obtained from training the perceptron and see how well they classify the test set, without actually training a new perceptron on the test set. Well, what do you know? We conveniently kept a copy of $w$ at each epoch.  

```{r testAccuracy}
W = res_train$W
n_epoch = 1:nrow(W)
accuracy_test = lapply(n_epoch, 
                       function(j){ 
                         sum(y_test * (x_test %*% W[j,]) > 0)/n_test #check accuracy via dot product
                         }
                       )
a.eTest = data.frame("accuracy.test" = unlist(accuracy_test),
                     "epoch.test" = n_epoch)
```

```{r plotTestAccuracy, echo=FALSE}
g1 +
  geom_line(data = a.eTest, aes(x = epoch.test, y = accuracy.test),
            color = "red") +
  geom_hline(yintercept = min(a.eTest$accuracy), color = "red", lty = "dashed") +
  geom_hline(yintercept = max(a.eTest$accuracy), color = "red", lty = "dashed") +
  ylim(0.90,1) +
  ggtitle("How Accuracy Changes w.r.t. Epoch (zoomed)", 
          subtitle = "Test Data added in red") +
  theme(plot.subtitle = element_text(color = "red")) 
```

The accuracy for the test data is generally lower than the accuracy for the training data, but the curves are very close; they tend to follow the same pattern of increasing and decreasing as well. 

(c) The confusion matrix is how I refer to my brain in private.  

```{r}
w = res_train$w # last w obtained from training the perceptron
our.results = rep(NA, length(y_test))
# get results on test data
our.results[(x_test %*% w > 0)] = 1
our.results[(x_test %*% w <= 0)] = -1
# initialize confusion matix
confusion = data.frame(matrix(NA, nrow = 2, ncol = 2))
colnames(confusion) = c("actual.yes", "actual.no")
rownames(confusion) = c("predicted.yes", "predicted.no")
# fill in confusion matrix
confusion["predicted.yes", "actual.yes"] = sum(our.results == 1 & our.results == y_test)
confusion["predicted.yes", "actual.no"] = sum(our.results == 1 & our.results != y_test)
confusion["predicted.no", "actual.no"] = sum(our.results == -1 & our.results == y_test)
confusion["predicted.no", "actual.yes"] = sum(our.results == -1 & our.results != y_test)
nameTheColumns = c("Actual Yes, y = 1", "Actual No, y = -1")
knitr::kable(confusion, 
             col.names = nameTheColumns,
             caption = "Confusion Matrix for Test Data")
```  
  
  
The accuracy after the last epoch is `r (confusion[1,1] + confusion[2,2])/n_test `.  

(d)
```{r oneThirdRun}
# instead of modifying the function and risk breaking it...
# we'll just extract the first third of the training data and run 1 epoch
third.train = floor(n_train/3)
x_train.third = x_train[1:third.train,]
y_train.third = y_train[1:third.train]
res_third = perceptron(x_train.third, y_train.third, I = 1)
w.prime = res_third$w
# For the sake of curiosity, we're going to bump I to 200; do not expect much change
res_train = perceptron(x_train, y_train, I = 200)
w.star = res_train$w
```  

```{r wPrimeVSw}
#check n_b values for the threshold b
n_b = 100
ROC.third.matrix = data.frame(matrix(NA, nrow = n_b, ncol = 2))
colnames(ROC.third.matrix) = c("TPR", "FPR")
ROC.full.matrix = data.frame(matrix(NA, nrow = n_b, ncol = 2))
colnames(ROC.full.matrix) = c("TPR", "FPR")
i = 1
for (b in seq(-5, 5, length.out = n_b)){
  results.third = sign(x_train %*% w.prime - b)
  results.full = sign(x_train %*% w.star - b)
  # get the confusion matrix entries
  TP.full = sum(results.full == 1 & results.full == y_train)
  FP.full = sum(results.full == 1 & results.full != y_train)
  FN.full = sum(results.full == -1 & results.full != y_train)
  TN.full = sum(results.full == -1 & results.full == y_train)
  
  TP.third = sum(results.third == 1 & results.third == y_train)
  FP.third = sum(results.third == 1 & results.third != y_train)
  FN.third = sum(results.third == -1 & results.third != y_train)
  TN.third = sum(results.third == -1 & results.third == y_train)
  #store in matrix for later plotting
  ROC.full.matrix[i,"TPR"] = TPR.full = TP.full/(TP.full + FN.full)
  ROC.full.matrix[i,"FPR"] = FPR.full = FP.full/(FP.full + TN.full)
  
  ROC.third.matrix[i,"TPR"] = TPR.third = TP.third/(TP.third + FN.third)
  ROC.third.matrix[i,"FPR"] = FPR.third = FP.third/(FP.third + TN.third)
  i = i + 1
}
```  

```{r ROCplotting, echo=FALSE}
ggplot(data = ROC.third.matrix, aes(x = FPR, y = TPR)) +
  geom_line(aes(color = "w.prime")) + 
  geom_line(data = ROC.full.matrix, aes(x = FPR, y = TPR, color = "w.star" )) +
  ggtitle("ROC Curves for w.prime (trained on 1/3 of data) and w.star (all data)")+
  geom_abline(intercept = 0, slope = 1, lty = "dashed") +
  theme_bw()
```

We see from the graph, that the ROC curve for $w^*$ is always greater than or equal to the curve for $w'$. Therefore, if we use use $w^*$, we have to accept fewer false positives to reach a desired amount of true positives.  

(e) Trapezoids with uneven intervals it is.

```{r AUCapproximation}
N = nrow(ROC.full.matrix) - 1
trapezoids.full = lapply(2:N, # get areas of all our trapezoids
                         function(i){
                           (ROC.full.matrix[i-1,"FPR"] - ROC.full.matrix[i,"FPR"])*
                             (ROC.full.matrix[i-1,"TPR"] + ROC.full.matrix[i, "TPR"])/2
                         })
AUC.star = sum(unlist(trapezoids.full))
trapezoids.third = lapply(2:N, # get areas of all our trapezoids
                         function(i){
                           (ROC.third.matrix[i-1,"FPR"] - ROC.third.matrix[i,"FPR"])*
                             (ROC.third.matrix[i-1,"TPR"] + ROC.third.matrix[i, "TPR"])/2
                         })
AUC.prime = sum(unlist(trapezoids.third))
```

AUC for $w^*$ is `r round(AUC.star,4)`.  
AUC for $w'$ is `r round(AUC.prime,4)`.  

These results are in line with the ROC curves we obtained in the previous section: $w^*$ has better performance than $w'$. They are very close, however, suggesting that the perceptron is converging quickly to its final state.  

##### (2) Balanced Winnow  

```{r balancedWinnow}
balanced_winnow = function(X, y, I, eta = 1){
  p = ncol(X)
  accuracy_epoch = data.frame(matrix(NA, nrow = I, ncol = 2))
  colnames(accuracy_epoch) = c("accuracy", "epoch")
  W_n = matrix(NA, nrow = I, ncol = ncol(X))
  W_p = matrix(NA, nrow = I, ncol = ncol(X))
  w_p = w_n = rep(.5/p, p)
  for (j in 1:I){
    for (i in 1:nrow(X)) {
      if (( y[i] * (w_p %*% X[i,] - w_n %*% X[i,]) ) <= 0 ){
        w_p = w_p * exp(eta*y[i]*X[i,])
        w_n = w_n * exp(-eta*y[i]*X[i,])
        s = sum(w_n + w_p)
        w_p = w_p/s
        w_n = w_n/s
      }
    }
    accuracy = sum((X %*% (w_p - w_n)) * y > 0)/length(y) # calculate propotion of correctly classified points
    accuracy_epoch[j, "accuracy"] = accuracy
    accuracy_epoch[j, "epoch"] = j 
    W_p[j,] = w_p
    W_n[j,] = w_n
  }
  return(list("a.e" = accuracy_epoch, "w_p" = w_p,
              "w_n" = w_n, "W_n" = W_n, "W_p" = W_p))
}
```

(a) Oh boy, another function, I can't wait to run it.  

```{r runningWinnow}
res_train = balanced_winnow(x_train, y_train, I = 100)
w_p = res_train$w_p
w_n = res_train$w_n
res_test = sign(x_test %*% (w_p - w_n))

```